{
  "best_metric": 0.08350870013237,
  "best_model_checkpoint": "./vector_gpt_trainer/checkpoint-2190",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 2190,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0228310502283105,
      "grad_norm": 0.7014606595039368,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.6621,
      "step": 10
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 0.6447400450706482,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.6024,
      "step": 20
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 0.5796657800674438,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.4929,
      "step": 30
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 0.49503105878829956,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.3554,
      "step": 40
    },
    {
      "epoch": 0.1141552511415525,
      "grad_norm": 0.33905935287475586,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.2269,
      "step": 50
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 0.1586281806230545,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.1451,
      "step": 60
    },
    {
      "epoch": 0.1598173515981735,
      "grad_norm": 0.08585666865110397,
      "learning_rate": 7e-05,
      "loss": 0.1126,
      "step": 70
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 0.08208057284355164,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.1024,
      "step": 80
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 0.05311610549688339,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0974,
      "step": 90
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 0.04658534377813339,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.0944,
      "step": 100
    },
    {
      "epoch": 0.2511415525114155,
      "grad_norm": 0.04288544878363609,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.093,
      "step": 110
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 0.03760422021150589,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0917,
      "step": 120
    },
    {
      "epoch": 0.2968036529680365,
      "grad_norm": 0.04081563651561737,
      "learning_rate": 0.00013,
      "loss": 0.091,
      "step": 130
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 0.041680410504341125,
      "learning_rate": 0.00014,
      "loss": 0.0901,
      "step": 140
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 0.04152705520391464,
      "learning_rate": 0.00015,
      "loss": 0.0899,
      "step": 150
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 0.040038324892520905,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.0893,
      "step": 160
    },
    {
      "epoch": 0.3881278538812785,
      "grad_norm": 0.04540444165468216,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0889,
      "step": 170
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 0.04245750978589058,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0888,
      "step": 180
    },
    {
      "epoch": 0.4337899543378995,
      "grad_norm": 0.04418991878628731,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0883,
      "step": 190
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 0.04693049564957619,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.088,
      "step": 200
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 0.04743055999279022,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.088,
      "step": 210
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 0.0456177182495594,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.0877,
      "step": 220
    },
    {
      "epoch": 0.5251141552511416,
      "grad_norm": 0.04629708081483841,
      "learning_rate": 0.00023,
      "loss": 0.0875,
      "step": 230
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 0.04998345300555229,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.0875,
      "step": 240
    },
    {
      "epoch": 0.5707762557077626,
      "grad_norm": 0.053405165672302246,
      "learning_rate": 0.00025,
      "loss": 0.0874,
      "step": 250
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 0.05220847576856613,
      "learning_rate": 0.00026,
      "loss": 0.0872,
      "step": 260
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 0.053919386118650436,
      "learning_rate": 0.00027,
      "loss": 0.0873,
      "step": 270
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 0.04843223839998245,
      "learning_rate": 0.00028,
      "loss": 0.0872,
      "step": 280
    },
    {
      "epoch": 0.6621004566210046,
      "grad_norm": 0.056774355471134186,
      "learning_rate": 0.00029,
      "loss": 0.087,
      "step": 290
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 0.052790042012929916,
      "learning_rate": 0.0003,
      "loss": 0.087,
      "step": 300
    },
    {
      "epoch": 0.7077625570776256,
      "grad_norm": 0.05339573323726654,
      "learning_rate": 0.0002999792782036658,
      "loss": 0.0869,
      "step": 310
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 0.05932050570845604,
      "learning_rate": 0.00029991711853990133,
      "loss": 0.0867,
      "step": 320
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 0.05456264317035675,
      "learning_rate": 0.0002998135381828383,
      "loss": 0.0866,
      "step": 330
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 0.057577285915613174,
      "learning_rate": 0.0002996685657507577,
      "loss": 0.0868,
      "step": 340
    },
    {
      "epoch": 0.7990867579908676,
      "grad_norm": 0.05524980649352074,
      "learning_rate": 0.0002994822412981823,
      "loss": 0.0865,
      "step": 350
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 0.05389640852808952,
      "learning_rate": 0.0002992546163048102,
      "loss": 0.0865,
      "step": 360
    },
    {
      "epoch": 0.8447488584474886,
      "grad_norm": 0.05503998324275017,
      "learning_rate": 0.00029898575366129145,
      "loss": 0.0865,
      "step": 370
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 0.061121415346860886,
      "learning_rate": 0.0002986757276518519,
      "loss": 0.0865,
      "step": 380
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 0.053654056042432785,
      "learning_rate": 0.0002983246239337692,
      "loss": 0.0866,
      "step": 390
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 0.05322769656777382,
      "learning_rate": 0.0002979325395137067,
      "loss": 0.0864,
      "step": 400
    },
    {
      "epoch": 0.9360730593607306,
      "grad_norm": 0.05464983358979225,
      "learning_rate": 0.0002974995827209109,
      "loss": 0.0864,
      "step": 410
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 0.05848462134599686,
      "learning_rate": 0.00029702587317728153,
      "loss": 0.0864,
      "step": 420
    },
    {
      "epoch": 0.9817351598173516,
      "grad_norm": 0.05429501459002495,
      "learning_rate": 0.0002965115417643212,
      "loss": 0.0864,
      "step": 430
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.08395755290985107,
      "eval_runtime": 0.932,
      "eval_samples_per_second": 1609.368,
      "eval_steps_per_second": 100.854,
      "step": 438
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 0.05869709327816963,
      "learning_rate": 0.00029595673058697357,
      "loss": 0.0864,
      "step": 440
    },
    {
      "epoch": 1.0273972602739727,
      "grad_norm": 0.05511347949504852,
      "learning_rate": 0.00029536159293436166,
      "loss": 0.0863,
      "step": 450
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 0.056224219501018524,
      "learning_rate": 0.0002947262932374352,
      "loss": 0.0863,
      "step": 460
    },
    {
      "epoch": 1.0730593607305936,
      "grad_norm": 0.053759511560201645,
      "learning_rate": 0.00029405100702353993,
      "loss": 0.0862,
      "step": 470
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 0.05812131613492966,
      "learning_rate": 0.00029333592086792107,
      "loss": 0.0864,
      "step": 480
    },
    {
      "epoch": 1.1187214611872145,
      "grad_norm": 0.0538807176053524,
      "learning_rate": 0.00029258123234217435,
      "loss": 0.0861,
      "step": 490
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 0.054167360067367554,
      "learning_rate": 0.0002917871499596587,
      "loss": 0.0861,
      "step": 500
    },
    {
      "epoch": 1.1643835616438356,
      "grad_norm": 0.05529690533876419,
      "learning_rate": 0.0002909538931178862,
      "loss": 0.0861,
      "step": 510
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 0.05208565294742584,
      "learning_rate": 0.0002900816920379045,
      "loss": 0.0861,
      "step": 520
    },
    {
      "epoch": 1.2100456621004567,
      "grad_norm": 0.05522146448493004,
      "learning_rate": 0.0002891707877006888,
      "loss": 0.086,
      "step": 530
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 0.04403793066740036,
      "learning_rate": 0.00028822143178056114,
      "loss": 0.0861,
      "step": 540
    },
    {
      "epoch": 1.2557077625570776,
      "grad_norm": 0.05374252423644066,
      "learning_rate": 0.00028723388657565514,
      "loss": 0.0861,
      "step": 550
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 0.05257044732570648,
      "learning_rate": 0.0002862084249354457,
      "loss": 0.0861,
      "step": 560
    },
    {
      "epoch": 1.3013698630136985,
      "grad_norm": 0.055285848677158356,
      "learning_rate": 0.0002851453301853628,
      "loss": 0.0861,
      "step": 570
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 0.053063444793224335,
      "learning_rate": 0.0002840448960485118,
      "loss": 0.0862,
      "step": 580
    },
    {
      "epoch": 1.3470319634703196,
      "grad_norm": 0.05252978578209877,
      "learning_rate": 0.00028290742656452014,
      "loss": 0.0859,
      "step": 590
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 0.05134035274386406,
      "learning_rate": 0.0002817332360055343,
      "loss": 0.0859,
      "step": 600
    },
    {
      "epoch": 1.3926940639269407,
      "grad_norm": 0.050674039870500565,
      "learning_rate": 0.00028052264878938914,
      "loss": 0.0859,
      "step": 610
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 0.05337957292795181,
      "learning_rate": 0.0002792759993899746,
      "loss": 0.0859,
      "step": 620
    },
    {
      "epoch": 1.4383561643835616,
      "grad_norm": 0.05380623787641525,
      "learning_rate": 0.0002779936322448233,
      "loss": 0.0859,
      "step": 630
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 0.053260598331689835,
      "learning_rate": 0.00027667590165994613,
      "loss": 0.0858,
      "step": 640
    },
    {
      "epoch": 1.4840182648401825,
      "grad_norm": 0.052016016095876694,
      "learning_rate": 0.00027532317171194046,
      "loss": 0.0857,
      "step": 650
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 0.053651027381420135,
      "learning_rate": 0.00027393581614739923,
      "loss": 0.0857,
      "step": 660
    },
    {
      "epoch": 1.5296803652968036,
      "grad_norm": 0.0499085858464241,
      "learning_rate": 0.0002725142182796485,
      "loss": 0.0858,
      "step": 670
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 0.05820319801568985,
      "learning_rate": 0.00027105877088284136,
      "loss": 0.0856,
      "step": 680
    },
    {
      "epoch": 1.5753424657534247,
      "grad_norm": 0.05285324901342392,
      "learning_rate": 0.0002695698760834384,
      "loss": 0.0858,
      "step": 690
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 0.048551492393016815,
      "learning_rate": 0.0002680479452491033,
      "loss": 0.0858,
      "step": 700
    },
    {
      "epoch": 1.6210045662100456,
      "grad_norm": 0.05460025742650032,
      "learning_rate": 0.00026649339887504673,
      "loss": 0.0855,
      "step": 710
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 0.04489608108997345,
      "learning_rate": 0.00026490666646784665,
      "loss": 0.0855,
      "step": 720
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04821191728115082,
      "learning_rate": 0.00026328818642678024,
      "loss": 0.0855,
      "step": 730
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 0.05570833757519722,
      "learning_rate": 0.0002616384059226977,
      "loss": 0.0858,
      "step": 740
    },
    {
      "epoch": 1.7123287671232876,
      "grad_norm": 0.0519939586520195,
      "learning_rate": 0.0002599577807744739,
      "loss": 0.0855,
      "step": 750
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 0.052075184881687164,
      "learning_rate": 0.0002582467753230693,
      "loss": 0.0855,
      "step": 760
    },
    {
      "epoch": 1.7579908675799087,
      "grad_norm": 0.054270606487989426,
      "learning_rate": 0.00025650586230323703,
      "loss": 0.0854,
      "step": 770
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 0.049415286630392075,
      "learning_rate": 0.0002547355227129109,
      "loss": 0.0855,
      "step": 780
    },
    {
      "epoch": 1.8036529680365296,
      "grad_norm": 0.048309411853551865,
      "learning_rate": 0.00025293624568031,
      "loss": 0.0853,
      "step": 790
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 0.05404258519411087,
      "learning_rate": 0.00025110852832879726,
      "loss": 0.0855,
      "step": 800
    },
    {
      "epoch": 1.8493150684931505,
      "grad_norm": 0.05125698074698448,
      "learning_rate": 0.0002492528756395289,
      "loss": 0.0855,
      "step": 810
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 0.04926792159676552,
      "learning_rate": 0.00024736980031193277,
      "loss": 0.0854,
      "step": 820
    },
    {
      "epoch": 1.8949771689497716,
      "grad_norm": 0.05119175836443901,
      "learning_rate": 0.0002454598226220545,
      "loss": 0.0854,
      "step": 830
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 0.05131164565682411,
      "learning_rate": 0.00024352347027881003,
      "loss": 0.0854,
      "step": 840
    },
    {
      "epoch": 1.9406392694063928,
      "grad_norm": 0.04746101424098015,
      "learning_rate": 0.00024156127827818446,
      "loss": 0.0852,
      "step": 850
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 0.05421648547053337,
      "learning_rate": 0.00023957378875541792,
      "loss": 0.0853,
      "step": 860
    },
    {
      "epoch": 1.9863013698630136,
      "grad_norm": 0.047218967229127884,
      "learning_rate": 0.00023756155083521846,
      "loss": 0.0852,
      "step": 870
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.0837802141904831,
      "eval_runtime": 0.9082,
      "eval_samples_per_second": 1651.611,
      "eval_steps_per_second": 103.501,
      "step": 876
    },
    {
      "epoch": 2.009132420091324,
      "grad_norm": 0.05063604563474655,
      "learning_rate": 0.00023552512048004425,
      "loss": 0.0853,
      "step": 880
    },
    {
      "epoch": 2.0319634703196345,
      "grad_norm": 0.047069188207387924,
      "learning_rate": 0.00023346506033649614,
      "loss": 0.0852,
      "step": 890
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 0.04814013093709946,
      "learning_rate": 0.0002313819395798639,
      "loss": 0.0852,
      "step": 900
    },
    {
      "epoch": 2.077625570776256,
      "grad_norm": 0.04797513410449028,
      "learning_rate": 0.0002292763337568683,
      "loss": 0.0851,
      "step": 910
    },
    {
      "epoch": 2.1004566210045663,
      "grad_norm": 0.04530209302902222,
      "learning_rate": 0.00022714882462664303,
      "loss": 0.0852,
      "step": 920
    },
    {
      "epoch": 2.1232876712328768,
      "grad_norm": 0.04917339235544205,
      "learning_rate": 0.000225,
      "loss": 0.085,
      "step": 930
    },
    {
      "epoch": 2.146118721461187,
      "grad_norm": 0.05111771821975708,
      "learning_rate": 0.0002228304535770228,
      "loss": 0.085,
      "step": 940
    },
    {
      "epoch": 2.1689497716894977,
      "grad_norm": 0.0485580712556839,
      "learning_rate": 0.00022064078478303302,
      "loss": 0.0851,
      "step": 950
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 0.04982468858361244,
      "learning_rate": 0.00021843159860297442,
      "loss": 0.085,
      "step": 960
    },
    {
      "epoch": 2.2146118721461185,
      "grad_norm": 0.04531390964984894,
      "learning_rate": 0.0002162035054142616,
      "loss": 0.085,
      "step": 970
    },
    {
      "epoch": 2.237442922374429,
      "grad_norm": 0.045902080833911896,
      "learning_rate": 0.00021395712081813807,
      "loss": 0.085,
      "step": 980
    },
    {
      "epoch": 2.26027397260274,
      "grad_norm": 0.04334638640284538,
      "learning_rate": 0.00021169306546959174,
      "loss": 0.0848,
      "step": 990
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 0.046188365668058395,
      "learning_rate": 0.0002094119649058735,
      "loss": 0.0849,
      "step": 1000
    },
    {
      "epoch": 2.3059360730593608,
      "grad_norm": 0.047983359545469284,
      "learning_rate": 0.00020711444937366742,
      "loss": 0.0849,
      "step": 1010
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 0.04488128051161766,
      "learning_rate": 0.00020480115365495926,
      "loss": 0.0847,
      "step": 1020
    },
    {
      "epoch": 2.3515981735159817,
      "grad_norm": 0.04394032806158066,
      "learning_rate": 0.00020247271689165222,
      "loss": 0.0848,
      "step": 1030
    },
    {
      "epoch": 2.374429223744292,
      "grad_norm": 0.04862150922417641,
      "learning_rate": 0.00020012978240897814,
      "loss": 0.0848,
      "step": 1040
    },
    {
      "epoch": 2.3972602739726026,
      "grad_norm": 0.046378083527088165,
      "learning_rate": 0.00019777299753775265,
      "loss": 0.0847,
      "step": 1050
    },
    {
      "epoch": 2.4200913242009134,
      "grad_norm": 0.047839704900979996,
      "learning_rate": 0.00019540301343552388,
      "loss": 0.0847,
      "step": 1060
    },
    {
      "epoch": 2.442922374429224,
      "grad_norm": 0.04453257471323013,
      "learning_rate": 0.00019302048490666353,
      "loss": 0.0846,
      "step": 1070
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 0.04582367464900017,
      "learning_rate": 0.00019062607022145078,
      "loss": 0.0847,
      "step": 1080
    },
    {
      "epoch": 2.4885844748858448,
      "grad_norm": 0.04200870543718338,
      "learning_rate": 0.0001882204309341982,
      "loss": 0.0845,
      "step": 1090
    },
    {
      "epoch": 2.5114155251141552,
      "grad_norm": 0.03999384865164757,
      "learning_rate": 0.00018580423170047068,
      "loss": 0.0846,
      "step": 1100
    },
    {
      "epoch": 2.5342465753424657,
      "grad_norm": 0.04461226984858513,
      "learning_rate": 0.00018337814009344714,
      "loss": 0.0846,
      "step": 1110
    },
    {
      "epoch": 2.557077625570776,
      "grad_norm": 0.04142630845308304,
      "learning_rate": 0.00018094282641947665,
      "loss": 0.0844,
      "step": 1120
    },
    {
      "epoch": 2.5799086757990866,
      "grad_norm": 0.04046958312392235,
      "learning_rate": 0.0001784989635328785,
      "loss": 0.0846,
      "step": 1130
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 0.045658741146326065,
      "learning_rate": 0.00017604722665003956,
      "loss": 0.0846,
      "step": 1140
    },
    {
      "epoch": 2.625570776255708,
      "grad_norm": 0.04260249435901642,
      "learning_rate": 0.0001735882931628578,
      "loss": 0.0846,
      "step": 1150
    },
    {
      "epoch": 2.6484018264840183,
      "grad_norm": 0.04315957799553871,
      "learning_rate": 0.0001711228424515855,
      "loss": 0.0844,
      "step": 1160
    },
    {
      "epoch": 2.671232876712329,
      "grad_norm": 0.044934824109077454,
      "learning_rate": 0.00016865155569712278,
      "loss": 0.0843,
      "step": 1170
    },
    {
      "epoch": 2.6940639269406392,
      "grad_norm": 0.047150254249572754,
      "learning_rate": 0.0001661751156928138,
      "loss": 0.0846,
      "step": 1180
    },
    {
      "epoch": 2.7168949771689497,
      "grad_norm": 0.04199504852294922,
      "learning_rate": 0.00016369420665579725,
      "loss": 0.0845,
      "step": 1190
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 0.03998827561736107,
      "learning_rate": 0.00016120951403796364,
      "loss": 0.0844,
      "step": 1200
    },
    {
      "epoch": 2.762557077625571,
      "grad_norm": 0.04337914288043976,
      "learning_rate": 0.00015872172433657134,
      "loss": 0.0843,
      "step": 1210
    },
    {
      "epoch": 2.7853881278538815,
      "grad_norm": 0.043126847594976425,
      "learning_rate": 0.00015623152490457402,
      "loss": 0.0843,
      "step": 1220
    },
    {
      "epoch": 2.808219178082192,
      "grad_norm": 0.0427987277507782,
      "learning_rate": 0.00015373960376071093,
      "loss": 0.0844,
      "step": 1230
    },
    {
      "epoch": 2.8310502283105023,
      "grad_norm": 0.04288368299603462,
      "learning_rate": 0.00015124664939941457,
      "loss": 0.0843,
      "step": 1240
    },
    {
      "epoch": 2.853881278538813,
      "grad_norm": 0.04431261494755745,
      "learning_rate": 0.00014875335060058543,
      "loss": 0.0843,
      "step": 1250
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 0.04016933962702751,
      "learning_rate": 0.00014626039623928907,
      "loss": 0.0842,
      "step": 1260
    },
    {
      "epoch": 2.8995433789954337,
      "grad_norm": 0.04040617123246193,
      "learning_rate": 0.00014376847509542598,
      "loss": 0.0843,
      "step": 1270
    },
    {
      "epoch": 2.922374429223744,
      "grad_norm": 0.043279580771923065,
      "learning_rate": 0.00014127827566342863,
      "loss": 0.0843,
      "step": 1280
    },
    {
      "epoch": 2.9452054794520546,
      "grad_norm": 0.0404050312936306,
      "learning_rate": 0.00013879048596203636,
      "loss": 0.0843,
      "step": 1290
    },
    {
      "epoch": 2.968036529680365,
      "grad_norm": 0.04045103117823601,
      "learning_rate": 0.00013630579334420275,
      "loss": 0.0843,
      "step": 1300
    },
    {
      "epoch": 2.990867579908676,
      "grad_norm": 0.040746744722127914,
      "learning_rate": 0.0001338248843071862,
      "loss": 0.0842,
      "step": 1310
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.08371248841285706,
      "eval_runtime": 0.8979,
      "eval_samples_per_second": 1670.562,
      "eval_steps_per_second": 104.689,
      "step": 1314
    },
    {
      "epoch": 3.0136986301369864,
      "grad_norm": 0.039808955043554306,
      "learning_rate": 0.00013134844430287725,
      "loss": 0.0843,
      "step": 1320
    },
    {
      "epoch": 3.036529680365297,
      "grad_norm": 0.039138372987508774,
      "learning_rate": 0.0001288771575484145,
      "loss": 0.0841,
      "step": 1330
    },
    {
      "epoch": 3.0593607305936072,
      "grad_norm": 0.03924277424812317,
      "learning_rate": 0.00012641170683714222,
      "loss": 0.0842,
      "step": 1340
    },
    {
      "epoch": 3.0821917808219177,
      "grad_norm": 0.038710128515958786,
      "learning_rate": 0.00012395277334996044,
      "loss": 0.0842,
      "step": 1350
    },
    {
      "epoch": 3.105022831050228,
      "grad_norm": 0.03848898783326149,
      "learning_rate": 0.00012150103646712153,
      "loss": 0.0842,
      "step": 1360
    },
    {
      "epoch": 3.127853881278539,
      "grad_norm": 0.038951873779296875,
      "learning_rate": 0.00011905717358052336,
      "loss": 0.0843,
      "step": 1370
    },
    {
      "epoch": 3.1506849315068495,
      "grad_norm": 0.038404256105422974,
      "learning_rate": 0.00011662185990655284,
      "loss": 0.084,
      "step": 1380
    },
    {
      "epoch": 3.17351598173516,
      "grad_norm": 0.03849658742547035,
      "learning_rate": 0.00011419576829952933,
      "loss": 0.084,
      "step": 1390
    },
    {
      "epoch": 3.1963470319634704,
      "grad_norm": 0.041688524186611176,
      "learning_rate": 0.0001117795690658018,
      "loss": 0.0841,
      "step": 1400
    },
    {
      "epoch": 3.219178082191781,
      "grad_norm": 0.03681042790412903,
      "learning_rate": 0.00010937392977854923,
      "loss": 0.084,
      "step": 1410
    },
    {
      "epoch": 3.2420091324200913,
      "grad_norm": 0.03888794407248497,
      "learning_rate": 0.0001069795150933365,
      "loss": 0.084,
      "step": 1420
    },
    {
      "epoch": 3.2648401826484017,
      "grad_norm": 0.03893210366368294,
      "learning_rate": 0.00010459698656447611,
      "loss": 0.0841,
      "step": 1430
    },
    {
      "epoch": 3.287671232876712,
      "grad_norm": 0.03444252163171768,
      "learning_rate": 0.00010222700246224735,
      "loss": 0.084,
      "step": 1440
    },
    {
      "epoch": 3.3105022831050226,
      "grad_norm": 0.036445554345846176,
      "learning_rate": 9.987021759102186e-05,
      "loss": 0.0841,
      "step": 1450
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.03871605545282364,
      "learning_rate": 9.752728310834782e-05,
      "loss": 0.084,
      "step": 1460
    },
    {
      "epoch": 3.356164383561644,
      "grad_norm": 0.037779320031404495,
      "learning_rate": 9.519884634504074e-05,
      "loss": 0.084,
      "step": 1470
    },
    {
      "epoch": 3.3789954337899544,
      "grad_norm": 0.03784193843603134,
      "learning_rate": 9.288555062633256e-05,
      "loss": 0.0838,
      "step": 1480
    },
    {
      "epoch": 3.401826484018265,
      "grad_norm": 0.03761769458651543,
      "learning_rate": 9.058803509412646e-05,
      "loss": 0.0839,
      "step": 1490
    },
    {
      "epoch": 3.4246575342465753,
      "grad_norm": 0.03742235526442528,
      "learning_rate": 8.830693453040829e-05,
      "loss": 0.084,
      "step": 1500
    },
    {
      "epoch": 3.4474885844748857,
      "grad_norm": 0.03859191760420799,
      "learning_rate": 8.604287918186193e-05,
      "loss": 0.0841,
      "step": 1510
    },
    {
      "epoch": 3.470319634703196,
      "grad_norm": 0.036442872136831284,
      "learning_rate": 8.37964945857384e-05,
      "loss": 0.084,
      "step": 1520
    },
    {
      "epoch": 3.493150684931507,
      "grad_norm": 0.03990023583173752,
      "learning_rate": 8.156840139702554e-05,
      "loss": 0.084,
      "step": 1530
    },
    {
      "epoch": 3.5159817351598175,
      "grad_norm": 0.03432462736964226,
      "learning_rate": 7.935921521696702e-05,
      "loss": 0.084,
      "step": 1540
    },
    {
      "epoch": 3.538812785388128,
      "grad_norm": 0.03523622453212738,
      "learning_rate": 7.716954642297714e-05,
      "loss": 0.0841,
      "step": 1550
    },
    {
      "epoch": 3.5616438356164384,
      "grad_norm": 0.03587774559855461,
      "learning_rate": 7.500000000000002e-05,
      "loss": 0.0839,
      "step": 1560
    },
    {
      "epoch": 3.584474885844749,
      "grad_norm": 0.0380009226500988,
      "learning_rate": 7.285117537335697e-05,
      "loss": 0.0838,
      "step": 1570
    },
    {
      "epoch": 3.6073059360730593,
      "grad_norm": 0.0351414754986763,
      "learning_rate": 7.072366624313169e-05,
      "loss": 0.0838,
      "step": 1580
    },
    {
      "epoch": 3.6301369863013697,
      "grad_norm": 0.03595321998000145,
      "learning_rate": 6.86180604201361e-05,
      "loss": 0.0839,
      "step": 1590
    },
    {
      "epoch": 3.65296803652968,
      "grad_norm": 0.03644472360610962,
      "learning_rate": 6.653493966350389e-05,
      "loss": 0.0839,
      "step": 1600
    },
    {
      "epoch": 3.6757990867579906,
      "grad_norm": 0.03387361019849777,
      "learning_rate": 6.447487951995573e-05,
      "loss": 0.084,
      "step": 1610
    },
    {
      "epoch": 3.6986301369863015,
      "grad_norm": 0.03566322103142738,
      "learning_rate": 6.243844916478155e-05,
      "loss": 0.0838,
      "step": 1620
    },
    {
      "epoch": 3.721461187214612,
      "grad_norm": 0.03679566830396652,
      "learning_rate": 6.04262112445821e-05,
      "loss": 0.0839,
      "step": 1630
    },
    {
      "epoch": 3.7442922374429224,
      "grad_norm": 0.035090427845716476,
      "learning_rate": 5.8438721721815536e-05,
      "loss": 0.0839,
      "step": 1640
    },
    {
      "epoch": 3.767123287671233,
      "grad_norm": 0.032037779688835144,
      "learning_rate": 5.6476529721189974e-05,
      "loss": 0.0838,
      "step": 1650
    },
    {
      "epoch": 3.7899543378995433,
      "grad_norm": 0.031182745471596718,
      "learning_rate": 5.4540177377945465e-05,
      "loss": 0.0837,
      "step": 1660
    },
    {
      "epoch": 3.8127853881278537,
      "grad_norm": 0.030269810929894447,
      "learning_rate": 5.263019968806721e-05,
      "loss": 0.0839,
      "step": 1670
    },
    {
      "epoch": 3.8356164383561646,
      "grad_norm": 0.03671688959002495,
      "learning_rate": 5.074712436047112e-05,
      "loss": 0.0837,
      "step": 1680
    },
    {
      "epoch": 3.858447488584475,
      "grad_norm": 0.0341559499502182,
      "learning_rate": 4.8891471671202675e-05,
      "loss": 0.0838,
      "step": 1690
    },
    {
      "epoch": 3.8812785388127855,
      "grad_norm": 0.03283022716641426,
      "learning_rate": 4.706375431968997e-05,
      "loss": 0.0839,
      "step": 1700
    },
    {
      "epoch": 3.904109589041096,
      "grad_norm": 0.033436119556427,
      "learning_rate": 4.526447728708908e-05,
      "loss": 0.0838,
      "step": 1710
    },
    {
      "epoch": 3.9269406392694064,
      "grad_norm": 0.03347719833254814,
      "learning_rate": 4.3494137696762955e-05,
      "loss": 0.0839,
      "step": 1720
    },
    {
      "epoch": 3.949771689497717,
      "grad_norm": 0.031502023339271545,
      "learning_rate": 4.175322467693068e-05,
      "loss": 0.0839,
      "step": 1730
    },
    {
      "epoch": 3.9726027397260273,
      "grad_norm": 0.03763985261321068,
      "learning_rate": 4.004221922552608e-05,
      "loss": 0.0839,
      "step": 1740
    },
    {
      "epoch": 3.9954337899543377,
      "grad_norm": 0.031839821487665176,
      "learning_rate": 3.8361594077302245e-05,
      "loss": 0.0839,
      "step": 1750
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.08356130123138428,
      "eval_runtime": 0.9398,
      "eval_samples_per_second": 1596.034,
      "eval_steps_per_second": 100.018,
      "step": 1752
    },
    {
      "epoch": 4.018264840182648,
      "grad_norm": 0.03335355967283249,
      "learning_rate": 3.67118135732198e-05,
      "loss": 0.0836,
      "step": 1760
    },
    {
      "epoch": 4.041095890410959,
      "grad_norm": 0.031416360288858414,
      "learning_rate": 3.509333353215331e-05,
      "loss": 0.0839,
      "step": 1770
    },
    {
      "epoch": 4.063926940639269,
      "grad_norm": 0.03177177533507347,
      "learning_rate": 3.350660112495324e-05,
      "loss": 0.0838,
      "step": 1780
    },
    {
      "epoch": 4.0867579908675795,
      "grad_norm": 0.03432665392756462,
      "learning_rate": 3.195205475089667e-05,
      "loss": 0.0839,
      "step": 1790
    },
    {
      "epoch": 4.109589041095891,
      "grad_norm": 0.033744681626558304,
      "learning_rate": 3.0430123916561672e-05,
      "loss": 0.0839,
      "step": 1800
    },
    {
      "epoch": 4.132420091324201,
      "grad_norm": 0.034237898886203766,
      "learning_rate": 2.8941229117158578e-05,
      "loss": 0.0838,
      "step": 1810
    },
    {
      "epoch": 4.155251141552512,
      "grad_norm": 0.03158213198184967,
      "learning_rate": 2.7485781720351518e-05,
      "loss": 0.0838,
      "step": 1820
    },
    {
      "epoch": 4.178082191780822,
      "grad_norm": 0.028988510370254517,
      "learning_rate": 2.6064183852600797e-05,
      "loss": 0.0837,
      "step": 1830
    },
    {
      "epoch": 4.200913242009133,
      "grad_norm": 0.03499296307563782,
      "learning_rate": 2.4676828288059558e-05,
      "loss": 0.0837,
      "step": 1840
    },
    {
      "epoch": 4.223744292237443,
      "grad_norm": 0.030725670978426933,
      "learning_rate": 2.332409834005382e-05,
      "loss": 0.0838,
      "step": 1850
    },
    {
      "epoch": 4.2465753424657535,
      "grad_norm": 0.03260426968336105,
      "learning_rate": 2.2006367755176655e-05,
      "loss": 0.0838,
      "step": 1860
    },
    {
      "epoch": 4.269406392694064,
      "grad_norm": 0.03143163025379181,
      "learning_rate": 2.0724000610025404e-05,
      "loss": 0.0838,
      "step": 1870
    },
    {
      "epoch": 4.292237442922374,
      "grad_norm": 0.02994273230433464,
      "learning_rate": 1.9477351210610877e-05,
      "loss": 0.0838,
      "step": 1880
    },
    {
      "epoch": 4.315068493150685,
      "grad_norm": 0.033157866448163986,
      "learning_rate": 1.82667639944657e-05,
      "loss": 0.0837,
      "step": 1890
    },
    {
      "epoch": 4.337899543378995,
      "grad_norm": 0.02942001260817051,
      "learning_rate": 1.709257343547986e-05,
      "loss": 0.0837,
      "step": 1900
    },
    {
      "epoch": 4.360730593607306,
      "grad_norm": 0.031013324856758118,
      "learning_rate": 1.5955103951488173e-05,
      "loss": 0.0838,
      "step": 1910
    },
    {
      "epoch": 4.383561643835616,
      "grad_norm": 0.030128417536616325,
      "learning_rate": 1.4854669814637143e-05,
      "loss": 0.0838,
      "step": 1920
    },
    {
      "epoch": 4.406392694063927,
      "grad_norm": 0.030937647446990013,
      "learning_rate": 1.3791575064554261e-05,
      "loss": 0.0837,
      "step": 1930
    },
    {
      "epoch": 4.429223744292237,
      "grad_norm": 0.028664080426096916,
      "learning_rate": 1.2766113424344814e-05,
      "loss": 0.0836,
      "step": 1940
    },
    {
      "epoch": 4.4520547945205475,
      "grad_norm": 0.030093368142843246,
      "learning_rate": 1.1778568219438839e-05,
      "loss": 0.0837,
      "step": 1950
    },
    {
      "epoch": 4.474885844748858,
      "grad_norm": 0.030178045853972435,
      "learning_rate": 1.0829212299311197e-05,
      "loss": 0.0839,
      "step": 1960
    },
    {
      "epoch": 4.497716894977169,
      "grad_norm": 0.03075605444610119,
      "learning_rate": 9.918307962095467e-06,
      "loss": 0.0837,
      "step": 1970
    },
    {
      "epoch": 4.52054794520548,
      "grad_norm": 0.030719591304659843,
      "learning_rate": 9.046106882113751e-06,
      "loss": 0.0837,
      "step": 1980
    },
    {
      "epoch": 4.54337899543379,
      "grad_norm": 0.030902989208698273,
      "learning_rate": 8.212850040341273e-06,
      "loss": 0.0836,
      "step": 1990
    },
    {
      "epoch": 4.566210045662101,
      "grad_norm": 0.029304953292012215,
      "learning_rate": 7.418767657825675e-06,
      "loss": 0.0838,
      "step": 2000
    },
    {
      "epoch": 4.589041095890411,
      "grad_norm": 0.029492534697055817,
      "learning_rate": 6.664079132078881e-06,
      "loss": 0.0838,
      "step": 2010
    },
    {
      "epoch": 4.6118721461187215,
      "grad_norm": 0.02778450772166252,
      "learning_rate": 5.948992976460037e-06,
      "loss": 0.0836,
      "step": 2020
    },
    {
      "epoch": 4.634703196347032,
      "grad_norm": 0.028776701539754868,
      "learning_rate": 5.2737067625647615e-06,
      "loss": 0.0837,
      "step": 2030
    },
    {
      "epoch": 4.657534246575342,
      "grad_norm": 0.03190697729587555,
      "learning_rate": 4.638407065638322e-06,
      "loss": 0.0837,
      "step": 2040
    },
    {
      "epoch": 4.680365296803653,
      "grad_norm": 0.027262719348073006,
      "learning_rate": 4.043269413026429e-06,
      "loss": 0.0838,
      "step": 2050
    },
    {
      "epoch": 4.703196347031963,
      "grad_norm": 0.028670784085989,
      "learning_rate": 3.4884582356788206e-06,
      "loss": 0.0838,
      "step": 2060
    },
    {
      "epoch": 4.726027397260274,
      "grad_norm": 0.02788560278713703,
      "learning_rate": 2.9741268227184255e-06,
      "loss": 0.0836,
      "step": 2070
    },
    {
      "epoch": 4.748858447488584,
      "grad_norm": 0.030133884400129318,
      "learning_rate": 2.5004172790890896e-06,
      "loss": 0.0836,
      "step": 2080
    },
    {
      "epoch": 4.771689497716895,
      "grad_norm": 0.02624376118183136,
      "learning_rate": 2.0674604862932654e-06,
      "loss": 0.0836,
      "step": 2090
    },
    {
      "epoch": 4.794520547945205,
      "grad_norm": 0.027690241113305092,
      "learning_rate": 1.6753760662307215e-06,
      "loss": 0.0838,
      "step": 2100
    },
    {
      "epoch": 4.817351598173516,
      "grad_norm": 0.026918012648820877,
      "learning_rate": 1.3242723481480976e-06,
      "loss": 0.0837,
      "step": 2110
    },
    {
      "epoch": 4.840182648401827,
      "grad_norm": 0.028337476775050163,
      "learning_rate": 1.0142463387085464e-06,
      "loss": 0.0838,
      "step": 2120
    },
    {
      "epoch": 4.863013698630137,
      "grad_norm": 0.027057062834501266,
      "learning_rate": 7.453836951897885e-07,
      "loss": 0.0836,
      "step": 2130
    },
    {
      "epoch": 4.885844748858448,
      "grad_norm": 0.02848581224679947,
      "learning_rate": 5.177587018176777e-07,
      "loss": 0.0837,
      "step": 2140
    },
    {
      "epoch": 4.908675799086758,
      "grad_norm": 0.030182942748069763,
      "learning_rate": 3.314342492422517e-07,
      "loss": 0.0837,
      "step": 2150
    },
    {
      "epoch": 4.931506849315069,
      "grad_norm": 0.028207212686538696,
      "learning_rate": 1.8646181716164831e-07,
      "loss": 0.0837,
      "step": 2160
    },
    {
      "epoch": 4.954337899543379,
      "grad_norm": 0.02798355370759964,
      "learning_rate": 8.288146009866602e-08,
      "loss": 0.0838,
      "step": 2170
    },
    {
      "epoch": 4.9771689497716896,
      "grad_norm": 0.02956966497004032,
      "learning_rate": 2.072179633414994e-08,
      "loss": 0.0838,
      "step": 2180
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.04209805652499199,
      "learning_rate": 0.0,
      "loss": 0.0837,
      "step": 2190
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.08350870013237,
      "eval_runtime": 0.9261,
      "eval_samples_per_second": 1619.7,
      "eval_steps_per_second": 101.501,
      "step": 2190
    }
  ],
  "logging_steps": 10,
  "max_steps": 2190,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
